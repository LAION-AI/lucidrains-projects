
# lucidrains-sumary

## Introduction

This repository gives an overview of the awesome projects created by [lucidrains](https://github.com/lucidrains) that we as LAION want to share with the community
in order to help people train new exciting models and do research with SOTA ML code. 

The whole LAION community started with crawling@home that became LAION-400M and later evolved into LAION-5B and at the same time lucidrains' awesome repository DALLE-pytorch,
a replication of OpenAI's Dall-E mode, that became more and more popular as we trained on CC-3m and CC-12m datasets and later on LAION-400M.

We are very thankful for the great work of lucidrains!

## Projects

| Repository Name | Year | Month | Day | Description | Projects |
| --- | --- | --- | --- | --- | --- |
| [coordinate-descent-hierarchical-memory](https://github.com/lucidrains/coordinate-descent-hierarchical-memory) | 2023 | May | 27 | Implementation of a hierarchical memory module using coordinate descent routing ||
| [flash-genomics-model](https://github.com/lucidrains/flash-genomics-model) | 2023 | May | 18 | My own attempt at a long context genomics model, leveraging recent advances in long context attention modeling (Flash Attention + other hierarchical methods) ||
| [soundstorm-pytorch](https://github.com/lucidrains/soundstorm-pytorch) | 2023 | May | 17 | Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch ||
| [MEGABYTE-pytorch](https://github.com/lucidrains/MEGABYTE-pytorch) | 2023 | May | 15 | Implementation of MEGABYTE, Predicting Million-byte Sequences with Multiscale Transformers, in Pytorch ||
| [MaMMUT-pytorch](https://github.com/lucidrains/MaMMUT-pytorch) | 2023 | May | 5 | Implementation of MaMMUT, a simple vision-encoder text-decoder architecture for multimodal tasks from Google, in Pytorch ||
| [recurrent-memory-transformer-pytorch](https://github.com/lucidrains/recurrent-memory-transformer-pytorch) | 2023 | April | 24 | Implementation of Recurrent Memory Transformer, Neurips 2022 paper, in Pytorch ||
| [mixture-of-attention](https://github.com/lucidrains/mixture-of-attention) | 2023 | April | 21 | Some personal experiments around routing tokens to different autoregressive attention, akin to mixture-of-experts ||
| [naturalspeech2-pytorch](https://github.com/lucidrains/naturalspeech2-pytorch) | 2023 | April | 19 | Implementation of Natural Speech 2, Zero-shot Speech and Singing Synthesizer, in Pytorch ||
| [simple-hierarchical-transformer](https://github.com/lucidrains/simple-hierarchical-transformer) | 2023 | April | 6 | Experiments around a simple idea for inducing multiple hierarchical predictive model within a GPT ||
| [neural-plexer-pytorch](https://github.com/lucidrains/neural-plexer-pytorch) | 2023 | April | 4 | Implementation of Nvidia's NeuralPlexer, for end-to-end differentiable design of functional small-molecules and ligand-binding proteins, in Pytorch ||
| [nucleotide-transformer](https://github.com/lucidrains/nucleotide-transformer) | 2023 | April | 3 | ðŸ§¬ Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics ||
| [coordinate-descent-attention](https://github.com/lucidrains/coordinate-descent-attention) | 2023 | March | 31 | Implementation of an Attention layer where each head can attend to more than just one token, using coordinate descent to pick topk ||
| [RFdiffusion](https://github.com/lucidrains/RFdiffusion) | 2023 | March | 31 | Code for running RFdiffusion ||
| [st-moe-pytorch](https://github.com/lucidrains/st-moe-pytorch) | 2023 | March | 26 | Implementation of ST-Moe, the latest incarnation of MoE after years of research at Brain, in Pytorch ||
| [CoLT5-attention](https://github.com/lucidrains/CoLT5-attention) | 2023 | March | 20 | Implementation of the conditionally routed attention in the CoLT5 architecture, in Pytorch ||
| [simplicial-attention](https://github.com/lucidrains/simplicial-attention) | 2023 | March | 12 | Explorations into the paper Simplicial Hopfield Networks, to see if any of the learning points can improve upon attention in a transformers setting ||
| [gigagan-pytorch](https://github.com/lucidrains/gigagan-pytorch) | 2023 | March | 10 | Implementation of GigaGAN, new SOTA GAN out of Adobe ||
| [bitsandbytes](https://github.com/lucidrains/bitsandbytes) | 2023 | March | 8 | 8-bit CUDA functions for PyTorch ||
| [TPDNE](https://github.com/lucidrains/TPDNE) | 2023 | March | 4 | Thispersondoesnotexist went down, so this time, while building it back up, I am going to open source all of it. ||
| [CLIP](https://github.com/lucidrains/CLIP) | 2023 | February | 26 | Contrastive Language-Image Pretraining ||
| [lion-pytorch](https://github.com/lucidrains/lion-pytorch) | 2023 | February | 15 | ðŸ¦ Lion, new optimizer discovered by Google Brain using genetic algorithms that is purportedly better than Adam(w), in Pytorch ||
| [toolformer-pytorch](https://github.com/lucidrains/toolformer-pytorch) | 2023 | February | 10 | Implementation of Toolformer, Language Models That Can Use Tools, by MetaAI ||
| [autoregressive-linear-attention-cuda](https://github.com/lucidrains/autoregressive-linear-attention-cuda) | 2023 | February | 7 | CUDA implementation of autoregressive linear attention, with all the latest research findings ||
| [block-recurrent-transformer-pytorch](https://github.com/lucidrains/block-recurrent-transformer-pytorch) | 2023 | February | 7 | Implementation of Block Recurrent Transformer - Pytorch ||
| [open_clip](https://github.com/lucidrains/open_clip) | 2023 | February | 3 | An open source implementation of CLIP. ||
| [rvq-vae-gpt](https://github.com/lucidrains/rvq-vae-gpt) | 2023 | January | 30 | My attempts at applying Soundstream design on learned tokenization of text and then applying hierarchical attention to text generation ||
| [musiclm-pytorch](https://github.com/lucidrains/musiclm-pytorch) | 2023 | January | 27 | Implementation of MusicLM, Google's new SOTA model for music generation using attention networks, in Pytorch ||
| [zorro-pytorch](https://github.com/lucidrains/zorro-pytorch) | 2023 | January | 26 | Implementation of Zorro, Masked Multimodal Transformer, in Pytorch ||
| [dreamerv3-pytorch](https://github.com/lucidrains/dreamerv3-pytorch) | 2023 | January | 12 | Implementation of Dreamer v3, Deepmind's first neural network that was able to learn to collect diamonds in Minecraft, in Pytorch ||
| [muse-maskgit-pytorch](https://github.com/lucidrains/muse-maskgit-pytorch) | 2023 | January | 3 | Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch ||
| [nim-tokenizer](https://github.com/lucidrains/nim-tokenizer) | 2022 | December | 30 | Implementation of a simple BPE tokenizer, but in Nim ||
| [equiformer-diffusion](https://github.com/lucidrains/equiformer-diffusion) | 2022 | December | 27 | Implementation of Denoising Diffusion for protein design, but using the new Equiformer (successor to SE3 Transformers) with some additional improvements ||
| [recurrent-interface-network-pytorch](https://github.com/lucidrains/recurrent-interface-network-pytorch) | 2022 | December | 23 | Implementation of Recurrent Interface Network (RIN), for highly efficient generation of images and video without cascading networks, in Pytorch ||
| [Nim](https://github.com/lucidrains/Nim) | 2022 | December | 21 | Nim is a statically typed compiled systems programming language. It combines successful concepts from mature languages like Python, Ada and Modula. Its design focuses on efficiency, expressiveness, and elegance (in that order of priority). ||
| [robotic-transformer-pytorch](https://github.com/lucidrains/robotic-transformer-pytorch) | 2022 | December | 13 | Implementation of RT1 (Robotic Transformer) in Pytorch ||
| [medical-chatgpt](https://github.com/lucidrains/medical-chatgpt) | 2022 | December | 10 | Implementation of ChatGPT, but tailored towards primary care medicine, with the reward being able to collect patient histories in a thorough and efficient manner and come up with a reasonable differential diagnosis ||
| [PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch) | 2022 | December | 9 | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM ||
| [memory-editable-transformer](https://github.com/lucidrains/memory-editable-transformer) | 2022 | December | 8 | My explorations into editing the knowledge and memories of an attention network ||
| [magic3d-pytorch](https://github.com/lucidrains/magic3d-pytorch) | 2022 | December | 5 | Implementation of Magic3D, Text to 3D content synthesis, in Pytorch ||
| [classifier-free-guidance-pytorch](https://github.com/lucidrains/classifier-free-guidance-pytorch) | 2022 | December | 3 | Implementation of Classifier Free Guidance in Pytorch, with emphasis on text conditioning, and flexibility to include multiple text embedding models ||
| [chroma-pytorch](https://github.com/lucidrains/chroma-pytorch) | 2022 | December | 2 | Implementation of Chroma, generative models of protein using DDPM and GNNs, in Pytorch ||
| [med-seg-diff-pytorch](https://github.com/lucidrains/med-seg-diff-pytorch) | 2022 | November | 23 | Implementation of MedSegDiff in Pytorch - SOTA medical segmentation using DDPM and filtering of features in fourier space ||
| [holodeck-pytorch](https://github.com/lucidrains/holodeck-pytorch) | 2022 | November | 13 | Implementation of a holodeck, written in Pytorch ||
| [equiformer-pytorch](https://github.com/lucidrains/equiformer-pytorch) | 2022 | October | 29 | Implementation of the Equiformer, SE3/E3 equivariant attention network that reaches new SOTA, and adopted for use by EquiFold for protein folding ||
| [make-a-video-pytorch](https://github.com/lucidrains/make-a-video-pytorch) | 2022 | September | 29 | Implementation of Make-A-Video, new SOTA text to video generator from Meta AI, in Pytorch ||
| [phenaki-pytorch](https://github.com/lucidrains/phenaki-pytorch) | 2022 | September | 29 | Implementation of Phenaki Video, which uses Mask GIT to produce text guided videos of up to 2 minutes in length, in Pytorch ||
| [Mega-pytorch](https://github.com/lucidrains/Mega-pytorch) | 2022 | September | 23 | Implementation of Mega, the Single-head Attention with Multi-headed EMA architecture that currently holds SOTA on Long Range Arena ||
| [audiolm-pytorch](https://github.com/lucidrains/audiolm-pytorch) | 2022 | September | 9 | Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch ||
| [Adan-pytorch](https://github.com/lucidrains/Adan-pytorch) | 2022 | August | 25 | Implementation of the Adan (ADAptive Nesterov momentum algorithm) Optimizer in Pytorch ||
| [JEPA-pytorch](https://github.com/lucidrains/JEPA-pytorch) | 2022 | August | 21 | Implementation of JEPA, Yann LeCun's vision of how AGI would be built, in Pytorch ||
| [bit-diffusion](https://github.com/lucidrains/bit-diffusion) | 2022 | August | 17 | Implementation of Bit Diffusion, Hinton's group's attempt at discrete denoising diffusion, in Pytorch ||
| [transframer-pytorch](https://github.com/lucidrains/transframer-pytorch) | 2022 | August | 17 | Implementation of Transframer, Deepmind's U-net + Transformer architecture for up to 30 seconds video generation, in Pytorch ||
| [pytorch](https://github.com/lucidrains/pytorch) | 2022 | August | 16 | Tensors and Dynamic neural networks in Python with strong GPU acceleration ||
| [flash-cosine-sim-attention](https://github.com/lucidrains/flash-cosine-sim-attention) | 2022 | August | 4 | Implementation of fused cosine similarity attention in the same style as Flash Attention ||
| [discrete-key-value-bottleneck-pytorch](https://github.com/lucidrains/discrete-key-value-bottleneck-pytorch) | 2022 | July | 25 | Implementation of Discrete Key / Value Bottleneck, in Pytorch ||
| [flash-attention-jax](https://github.com/lucidrains/flash-attention-jax) | 2022 | July | 12 | Implementation of Flash Attention in Jax ||
| [flash-attention](https://github.com/lucidrains/flash-attention) | 2022 | July | 7 | Fast and memory-efficient exact attention ||
| [RITA](https://github.com/lucidrains/RITA) | 2022 | July | 3 | RITA is a family of autoregressive protein models, developed by LightOn in collaboration with the OATML group at Oxford and the Debora Marks Lab at Harvard. ||
| [gated-state-spaces-pytorch](https://github.com/lucidrains/gated-state-spaces-pytorch) | 2022 | June | 29 | Implementation of Gated State Spaces, from the paper "Long Range Language Modeling via Gated State Spaces", in Pytorch ||
| [parti-pytorch](https://github.com/lucidrains/parti-pytorch) | 2022 | June | 22 | Implementation of Parti, Google's pure attention-based text-to-image neural network, in Pytorch ||
| [ema-pytorch](https://github.com/lucidrains/ema-pytorch) | 2022 | June | 20 | A simple way to keep track of an Exponential Moving Average (EMA) version of your pytorch model ||
| [bolt](https://github.com/lucidrains/bolt) | 2022 | June | 18 | 10x faster matrix and vector operations ||
| [metaformer-gpt](https://github.com/lucidrains/metaformer-gpt) | 2022 | June | 18 | Implementation of Metaformer, but in an autoregressive manner ||
| [perceiver-ar-pytorch](https://github.com/lucidrains/perceiver-ar-pytorch) | 2022 | June | 18 | Implementation of Perceiver AR, Deepmind's new long-context attention network based on Perceiver architecture, in Pytorch ||
| [VN-transformer](https://github.com/lucidrains/VN-transformer) | 2022 | June | 12 | A Transformer made of Rotation-equivariant Attention using Vector Neurons ||
| [tranception-pytorch](https://github.com/lucidrains/tranception-pytorch) | 2022 | June | 2 | Implementation of Tranception, an attention network, paired with retrieval, that is SOTA for protein fitness prediction ||
| [ddpm-ipa-protein-generation](https://github.com/lucidrains/ddpm-ipa-protein-generation) | 2022 | May | 31 | Implementation of the DDPM + IPA (invariant point attention) for protein generation, as outlined in the paper "Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models" ||
| [insertion-deletion-ddpm](https://github.com/lucidrains/insertion-deletion-ddpm) | 2022 | May | 31 | Implementation of Insertion-deletion Denoising Diffusion Probabilistic Models ||
| [flexible-diffusion-modeling-videos-pytorch](https://github.com/lucidrains/flexible-diffusion-modeling-videos-pytorch) | 2022 | May | 28 | Implementation of the video diffusion model and training scheme presented in the paper, Flexible Diffusion Modeling of Long Videos, in Pytorch ||
| [imagen-pytorch](https://github.com/lucidrains/imagen-pytorch) | 2022 | May | 23 | Implementation of Imagen, Google's Text-to-Image Neural Network, in Pytorch ||
| [compositional-attention-pytorch](https://github.com/lucidrains/compositional-attention-pytorch) | 2022 | May | 9 | Implementation of "compositional attention" from MILA, a multi-head attention variant that is reframed as a two-step attention process with disentangled search and retrieval head aggregation, in Pytorch ||
| [differentiable-SDF-pytorch](https://github.com/lucidrains/differentiable-SDF-pytorch) | 2022 | May | 9 | Implementation of Differentiable Sign-Distance Function Rendering - in Pytorch ||
| [CoCa-pytorch](https://github.com/lucidrains/CoCa-pytorch) | 2022 | May | 5 | Implementation of CoCa, Contrastive Captioners are Image-Text Foundation Models, in Pytorch ||
| [ResizeRight](https://github.com/lucidrains/ResizeRight) | 2022 | May | 5 | The correct way to resize images or tensors. For Numpy or Pytorch (differentiable). ||
| [retrieval-augmented-ddpm](https://github.com/lucidrains/retrieval-augmented-ddpm) | 2022 | May | 1 | Implementation of Retrieval-Augmented Denoising Diffusion Probabilistic Models in Pytorch ||
| [flamingo-pytorch](https://github.com/lucidrains/flamingo-pytorch) | 2022 | April | 28 | Implementation of ðŸ¦© Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch ||
| [einops-exts](https://github.com/lucidrains/einops-exts) | 2022 | April | 9 | Implementation of some personal helper functions for Einops, my most favorite tensor manipulation library â¤ï¸ ||
| [PaLM-jax](https://github.com/lucidrains/PaLM-jax) | 2022 | April | 8 | Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways - in Jax (Equinox framework) ||
| [video-diffusion-pytorch](https://github.com/lucidrains/video-diffusion-pytorch) | 2022 | April | 8 | Implementation of Video Diffusion Models, Jonathan Ho's new paper extending DDPMs to Video Generation - in Pytorch ||
| [DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch) | 2022 | April | 7 | Implementation of DALL-E 2, OpenAI's updated text-to-image synthesis neural network,  in Pytorch ||
| [attention](https://github.com/lucidrains/attention) | 2022 | April | 5 | This repository will house a visualization that will attempt to convey instant enlightenment of how Attention works to someone not working in artificial intelligence, with 3Blue1Brown as inspiration ||
| [RaveForce](https://github.com/lucidrains/RaveForce) | 2022 | April | 5 | RaveForce - An OpenAI Gym style toolkit for music generation experiments. ||
| [PaLM-pytorch](https://github.com/lucidrains/PaLM-pytorch) | 2022 | April | 4 | Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways ||
| [ITTR-pytorch](https://github.com/lucidrains/ITTR-pytorch) | 2022 | April | 1 | Implementation of the Hybrid Perception Block and Dual-Pruned Self-Attention block from the ITTR paper for Image to Image Translation using Transformers ||
| [neural-sequence-chunkers-pytorch](https://github.com/lucidrains/neural-sequence-chunkers-pytorch) | 2022 | April | 1 | Implementation of the Neural Sequence Chunker, Schmidhuber paper back from 1991, in the context of Attention and Transformers ||
| [tableformer-pytorch](https://github.com/lucidrains/tableformer-pytorch) | 2022 | March | 29 | Implementation of TableFormer, Robust Transformer Modeling for Table-Text Encoding, in Pytorch ||
| [FLASH-pytorch](https://github.com/lucidrains/FLASH-pytorch) | 2022 | March | 28 | Implementation of the Transformer variant proposed in "Transformer Quality in Linear Time" ||
| [bidirectional-cross-attention](https://github.com/lucidrains/bidirectional-cross-attention) | 2022 | March | 27 | A simple cross attention that updates both the source and target in one step ||
| [keops](https://github.com/lucidrains/keops) | 2022 | March | 27 | KErnel OPerationS, on CPUs and GPUs, with autodiff and without memory overflows ||
| [x-unet](https://github.com/lucidrains/x-unet) | 2022 | March | 23 | Implementation of a U-net complete with efficient attention as well as the latest research findings ||
| [memorizing-transformers-pytorch](https://github.com/lucidrains/memorizing-transformers-pytorch) | 2022 | March | 21 | Implementation of Memorizing Transformers (ICLR 2022), attention net augmented with indexing and retrieval of memories using approximate nearest neighbors, in Pytorch ||
| [binding-ddg-predictor](https://github.com/lucidrains/binding-ddg-predictor) | 2022 | March | 19 | open source repository ||
| [deformable-attention](https://github.com/lucidrains/deformable-attention) | 2022 | March | 17 | Implementation of Deformable Attention in Pytorch from the paper "Vision Transformer with Deformable Attention" ||
| [RQ-Transformer](https://github.com/lucidrains/RQ-Transformer) | 2022 | March | 11 | Implementation of RQ Transformer, proposed in the paper "Autoregressive Image Generation using Residual Quantization" ||
| [ffcv](https://github.com/lucidrains/ffcv) | 2022 | March | 9 | FFCV: Fast Forward Computer Vision (and other ML workloads!) ||
| [einops](https://github.com/lucidrains/einops) | 2022 | March | 3 | Deep learning operations reinvented (for pytorch, tensorflow, jax and others) ||
| [memory-efficient-attention-pytorch](https://github.com/lucidrains/memory-efficient-attention-pytorch) | 2022 | March | 3 | Implementation of a memory efficient multi-head attention as proposed in the paper, "Self-attention Does Not Need O(nÂ²) Memory" ||
| [ColabFold](https://github.com/lucidrains/ColabFold) | 2022 | February | 18 | Making Protein folding accessible to all via Google Colab! ||
| [ETSformer-pytorch](https://github.com/lucidrains/ETSformer-pytorch) | 2022 | February | 5 | Implementation of ETSformer, state of the art time-series Transformer, in Pytorch ||
| [equinox](https://github.com/lucidrains/equinox) | 2022 | February | 1 | Callable PyTrees and filtered JIT/grad transformations => neural networks in JAX. ||
| [logavgexp-torch](https://github.com/lucidrains/logavgexp-torch) | 2022 | January | 31 | Implementation of LogAvgExp for Pytorch ||
| [anymal-belief-state-encoder-decoder-pytorch](https://github.com/lucidrains/anymal-belief-state-encoder-decoder-pytorch) | 2022 | January | 26 | Implementation of the Belief State Encoder / Decoder in the new breakthrough robotics paper from ETH ZÃ¼rich ||
| [RETRO-pytorch](https://github.com/lucidrains/RETRO-pytorch) | 2022 | January | 15 | Implementation of RETRO, Deepmind's Retrieval based Attention net, in Pytorch ||
| [rela-transformer](https://github.com/lucidrains/rela-transformer) | 2022 | January | 10 | Implementation of a Transformer using ReLA (Rectified Linear Attention) from https://arxiv.org/abs/2104.07012 ||
| [enformer-tensorflow-sonnet-training-script](https://github.com/lucidrains/enformer-tensorflow-sonnet-training-script) | 2022 | January | 5 | The full training script for Enformer - Tensorflow Sonnet ||
| [staged-recipes](https://github.com/lucidrains/staged-recipes) | 2021 | December | 22 | A place to submit conda recipes before they become fully fledged conda-forge feedstocks ||
| [DeepBind](https://github.com/lucidrains/DeepBind) | 2021 | December | 8 | Training and testing of DeepBind models. ||
| [tf-bind-transformer](https://github.com/lucidrains/tf-bind-transformer) | 2021 | December | 8 | A repository with exploration into using transformers to predict DNA â†” transcription factor binding ||
| [ANANSE](https://github.com/lucidrains/ANANSE) | 2021 | December | 7 | Prediction of key transcription factors in cell fate determination using enhancer networks. See full ANANSE documentation for detailed installation instructions and usage examples. http://anansepy.readthedocs.io ||
| [x-clip](https://github.com/lucidrains/x-clip) | 2021 | December | 1 | A concise but complete implementation of CLIP with various experimental improvements from recent papers ||
| [n-grammer-pytorch](https://github.com/lucidrains/n-grammer-pytorch) | 2021 | November | 28 | Implementation of N-Grammer, augmenting Transformers with latent n-grams, in Pytorch ||
| [nuwa-pytorch](https://github.com/lucidrains/nuwa-pytorch) | 2021 | November | 28 | Implementation of NÃœWA, state of the art attention network for text to video synthesis, in Pytorch ||
| [panoptic-transformer](https://github.com/lucidrains/panoptic-transformer) | 2021 | November | 22 | Another attempt at a long-context / efficient transformer by me ||
| [uniformer-pytorch](https://github.com/lucidrains/uniformer-pytorch) | 2021 | November | 13 | Implementation of Uniformer, a simple attention and 3d convolutional net that achieved SOTA in a number of video classification tasks, debuted in ICLR 2022 ||
| [hourglass-transformer-pytorch](https://github.com/lucidrains/hourglass-transformer-pytorch) | 2021 | November | 8 | Implementation of Hourglass Transformer, in Pytorch, from Google and OpenAI ||
| [mujoco](https://github.com/lucidrains/mujoco) | 2021 | October | 31 | Multi-Joint dynamics with Contact. A general purpose physics simulator. ||
| [deepmind-research](https://github.com/lucidrains/deepmind-research) | 2021 | October | 26 | This repository contains implementations and illustrative code to accompany DeepMind publications ||
| [jax2torch](https://github.com/lucidrains/jax2torch) | 2021 | October | 26 | Use Jax functions in Pytorch ||
| [enformer-pytorch](https://github.com/lucidrains/enformer-pytorch) | 2021 | October | 23 | Implementation of Enformer, Deepmind's attention network for predicting gene expression, in Pytorch ||
| [lucidrains](https://github.com/lucidrains/lucidrains) | 2021 | October | 1 | Config files for my GitHub profile. ||
| [remixer-pytorch](https://github.com/lucidrains/remixer-pytorch) | 2021 | September | 24 | Implementation of the Remixer Block from the Remixer paper, in Pytorch ||
| [triton](https://github.com/lucidrains/triton) | 2021 | September | 20 | Development repository for the Triton language and compiler ||
| [rgn2-replica](https://github.com/lucidrains/rgn2-replica) | 2021 | September | 19 | Replication attempt for the Protein Folding Model described in https://www.biorxiv.org/content/10.1101/2021.08.02.454840v1 ||
| [HTM-pytorch](https://github.com/lucidrains/HTM-pytorch) | 2021 | September | 14 | Implementation of Hierarchical Transformer Memory (HTM) for Pytorch ||
| [triton-transformer](https://github.com/lucidrains/triton-transformer) | 2021 | September | 8 | Implementation of a Transformer, but completely in Triton ||
| [bonito](https://github.com/lucidrains/bonito) | 2021 | September | 1 | A PyTorch Basecaller for Oxford Nanopore Reads ||
| [ponder-transformer](https://github.com/lucidrains/ponder-transformer) | 2021 | August | 25 | Implementation of a Transformer that Ponders, using the scheme from the PonderNet paper ||
| [fast-transformer-pytorch](https://github.com/lucidrains/fast-transformer-pytorch) | 2021 | August | 23 | Implementation of Fast Transformer in Pytorch ||
| [token-shift-gpt](https://github.com/lucidrains/token-shift-gpt) | 2021 | August | 17 | Implementation of Token Shift GPT - An autoregressive model that solely relies on shifting the sequence space for mixing ||
| [lucidrains.github.io](https://github.com/lucidrains/lucidrains.github.io) | 2021 | August | 3 | None ||
| [multistream-transformers](https://github.com/lucidrains/multistream-transformers) | 2021 | July | 29 | Implementation of Multistream Transformers in Pytorch ||
| [h-transformer-1d](https://github.com/lucidrains/h-transformer-1d) | 2021 | July | 28 | Implementation of H-Transformer-1D, Hierarchical Attention for Sequence Learning ||
| [triangle-multiplicative-module](https://github.com/lucidrains/triangle-multiplicative-module) | 2021 | July | 19 | Implementation of the Triangle Multiplicative module, used in Alphafold2 as an efficient way to mix rows or columns of a 2d feature map, as a standalone package for Pytorch ||
| [invariant-point-attention](https://github.com/lucidrains/invariant-point-attention) | 2021 | July | 16 | Implementation of Invariant Point Attention, used for coordinate refinement in the structure module of Alphafold2, as a standalone Pytorch module ||
| [long-short-transformer](https://github.com/lucidrains/long-short-transformer) | 2021 | July | 7 | Implementation of Long-Short Transformer, combining local and global inductive biases for attention over long sequences, in Pytorch ||
| [charformer-pytorch](https://github.com/lucidrains/charformer-pytorch) | 2021 | June | 30 | Implementation of the GBST block from the Charformer paper, in Pytorch ||
| [rotary-embedding-torch](https://github.com/lucidrains/rotary-embedding-torch) | 2021 | June | 29 | Implementation of Rotary Embeddings, from the Roformer paper, in Pytorch ||
| [graph-transformer-pytorch](https://github.com/lucidrains/graph-transformer-pytorch) | 2021 | June | 18 | Implementation of Graph Transformer in Pytorch, for potential use in replicating Alphafold2 ||
| [uformer-pytorch](https://github.com/lucidrains/uformer-pytorch) | 2021 | June | 17 | Implementation of Uformer, Attention-based Unet, in Pytorch ||
| [ddpm-proteins](https://github.com/lucidrains/ddpm-proteins) | 2021 | June | 14 | A denoising diffusion probabilistic model (DDPM) tailored for conditional generation of protein distograms ||
| [ddpm-jax](https://github.com/lucidrains/ddpm-jax) | 2021 | June | 13 | None ||
| [NWT-pytorch](https://github.com/lucidrains/NWT-pytorch) | 2021 | June | 9 | Implementation of NWT, audio-to-video generation, in Pytorch ||
| [progen](https://github.com/lucidrains/progen) | 2021 | June | 9 | Implementation and replication of ProGen, Language Modeling for Protein Generation, in Jax ||
| [esbn-transformer](https://github.com/lucidrains/esbn-transformer) | 2021 | June | 7 | An attempt to merge ESBN with Transformers, to endow Transformers with the ability to emergently bind symbols ||
| [segformer-pytorch](https://github.com/lucidrains/segformer-pytorch) | 2021 | June | 6 | Implementation of Segformer, Attention + MLP neural network for segmentation, in Pytorch ||
| [local-attention-flax](https://github.com/lucidrains/local-attention-flax) | 2021 | May | 26 | Local Attention - Flax module for Jax ||
| [protein-bert-pytorch](https://github.com/lucidrains/protein-bert-pytorch) | 2021 | May | 26 | Implementation of ProteinBERT in Pytorch ||
| [mlp-gpt-jax](https://github.com/lucidrains/mlp-gpt-jax) | 2021 | May | 21 | A GPT, made only of MLPs, in Jax ||
| [g-mlp-gpt](https://github.com/lucidrains/g-mlp-gpt) | 2021 | May | 20 | GPT, but made only out of MLPs ||
| [g-mlp-pytorch](https://github.com/lucidrains/g-mlp-pytorch) | 2021 | May | 18 | Implementation of gMLP, an all-MLP replacement for Transformers, in Pytorch ||
| [res-mlp-pytorch](https://github.com/lucidrains/res-mlp-pytorch) | 2021 | May | 10 | Implementation of ResMLP, an all MLP solution to image classification, in Pytorch ||
| [mlp-mixer-pytorch](https://github.com/lucidrains/mlp-mixer-pytorch) | 2021 | May | 5 | An All-MLP solution for Vision, from Google AI ||
| [CLAP](https://github.com/lucidrains/CLAP) | 2021 | April | 16 | Contrastive Language-Audio Pretraining ||
| [clasp](https://github.com/lucidrains/clasp) | 2021 | April | 4 | CLASP - Contrastive Language-Aminoacid Sequence Pretraining ||
| [DALLE-mtf](https://github.com/lucidrains/DALLE-mtf) | 2021 | March | 30 | Open-AI's DALL-E for large scale training in mesh-tensorflow. ||
| [STAM-pytorch](https://github.com/lucidrains/STAM-pytorch) | 2021 | March | 28 | Implementation of STAM (Space Time Attention Model), a pure and simple attention model that reaches SOTA for video classification ||
| [halonet-pytorch](https://github.com/lucidrains/halonet-pytorch) | 2021 | March | 24 | Implementation of the ðŸ˜‡ Attention layer from the paper, Scaling Local Self-Attention For Parameter Efficient Visual Backbones ||
| [transganformer](https://github.com/lucidrains/transganformer) | 2021 | March | 11 | Implementation of TransGanFormer, an all-attention GAN that combines the finding from the recent GanFormer and TransGan paper ||
| [perceiver-pytorch](https://github.com/lucidrains/perceiver-pytorch) | 2021 | March | 5 | Implementation of Perceiver, General Perception with Iterative Attention, in Pytorch ||
| [mesh](https://github.com/lucidrains/mesh) | 2021 | March | 3 | Mesh TensorFlow: Model Parallelism Made Easier ||
| [coco-lm-pytorch](https://github.com/lucidrains/coco-lm-pytorch) | 2021 | March | 2 | Implementation of COCO-LM, Correcting and Contrasting Text Sequences for Language Model Pretraining, in Pytorch ||
| [glom-pytorch](https://github.com/lucidrains/glom-pytorch) | 2021 | March | 2 | An attempt at the implementation of Glom, Geoffrey Hinton's new idea that integrates concepts from neural fields, top-down-bottom-up processing, and attention (consensus between columns), for emergent part-whole heirarchies from data ||
| [omninet-pytorch](https://github.com/lucidrains/omninet-pytorch) | 2021 | March | 2 | Implementation of OmniNet, Omnidirectional Representations from Transformers, in Pytorch ||
| [transformer-in-transformer](https://github.com/lucidrains/transformer-in-transformer) | 2021 | March | 2 | Implementation of Transformer in Transformer, pixel level attention paired with patch level attention for image classification, in Pytorch ||
| [En-transformer](https://github.com/lucidrains/En-transformer) | 2021 | February | 27 | Implementation of E(n)-Transformer, which extends the ideas of Welling's E(n)-Equivariant Graph Neural Network to attention ||
| [egnn-pytorch](https://github.com/lucidrains/egnn-pytorch) | 2021 | February | 26 | Implementation of E(n)-Equivariant Graph Neural Networks, in Pytorch ||
| [nystrom-attention](https://github.com/lucidrains/nystrom-attention) | 2021 | February | 11 | Implementation of NystrÃ¶m Self-attention, from the paper NystrÃ¶mformer ||
| [TimeSformer-pytorch](https://github.com/lucidrains/TimeSformer-pytorch) | 2021 | February | 11 | Implementation of TimeSformer from Facebook AI, a pure attention-based solution for video classification ||
| [tr-rosetta-pytorch](https://github.com/lucidrains/tr-rosetta-pytorch) | 2021 | February | 4 | Implementation of trRosetta and trDesign for Pytorch, made into a convenient package, for protein structure prediction and design ||
| [feedback-transformer-pytorch](https://github.com/lucidrains/feedback-transformer-pytorch) | 2021 | February | 2 | Implementation of Feedback Transformer in Pytorch ||
| [bottleneck-transformer-pytorch](https://github.com/lucidrains/bottleneck-transformer-pytorch) | 2021 | January | 28 | Implementation of Bottleneck Transformer in Pytorch ||
| [big-sleep](https://github.com/lucidrains/big-sleep) | 2021 | January | 18 | A simple command line tool for text to image generation, using OpenAI's CLIP and a BigGAN. Technique was originally created by https://twitter.com/advadnoun ||
| [deep-daze](https://github.com/lucidrains/deep-daze) | 2021 | January | 17 | Simple command line tool for text to image generation using OpenAI's CLIP and Siren (Implicit neural representation network). Technique was originally created by https://twitter.com/advadnoun ||
| [geometric-vector-perceptron](https://github.com/lucidrains/geometric-vector-perceptron) | 2021 | January | 13 | Implementation of Geometric Vector Perceptron, a simple circuit for 3d rotation equivariance for learning over large biomolecules, in Pytorch. Idea proposed and accepted at ICLR 2021 ||
| [se3-transformer-pytorch](https://github.com/lucidrains/se3-transformer-pytorch) | 2021 | January | 9 | Implementation of SE3-Transformers for Equivariant Self-Attention, in Pytorch. This specific repository is geared towards integration with eventual Alphafold2 replication. ||
| [DALLE-pytorch](https://github.com/lucidrains/DALLE-pytorch) | 2021 | January | 5 | Implementation / replication of DALL-E, OpenAI's Text to Image Transformer, in Pytorch ||
| [ESBN-pytorch](https://github.com/lucidrains/ESBN-pytorch) | 2021 | January | 1 | Usable implementation of Emerging Symbol Binding Network (ESBN), in Pytorch ||
| [lie-transformer-pytorch](https://github.com/lucidrains/lie-transformer-pytorch) | 2020 | December | 22 | Implementation of Lie Transformer, Equivariant Self-Attention, in Pytorch ||
| [point-transformer-pytorch](https://github.com/lucidrains/point-transformer-pytorch) | 2020 | December | 18 | Implementation of the Point Transformer layer, in Pytorch ||
| [tab-transformer-pytorch](https://github.com/lucidrains/tab-transformer-pytorch) | 2020 | December | 15 | Implementation of TabTransformer, attention network for tabular data, in Pytorch ||
| [cross-transformers-pytorch](https://github.com/lucidrains/cross-transformers-pytorch) | 2020 | December | 11 | Implementation of Cross Transformer for spatially-aware few-shot transfer, in Pytorch ||
| [distilled-retriever-pytorch](https://github.com/lucidrains/distilled-retriever-pytorch) | 2020 | December | 11 | Implementation of the retriever distillation procedure as outlined in the paper "Distilling Knowledge from Reader to Retriever" ||
| [adjacent-attention-network](https://github.com/lucidrains/adjacent-attention-network) | 2020 | December | 10 | Graph neural network message passing reframed as a Transformer with local attention ||
| [pi-GAN-pytorch](https://github.com/lucidrains/pi-GAN-pytorch) | 2020 | December | 4 | Implementation of Ï€-GAN, for 3d-aware image synthesis, in Pytorch ||
| [alphafold2](https://github.com/lucidrains/alphafold2) | 2020 | December | 1 | To eventually become an unofficial Pytorch implementation / replication of Alphafold2, as details of the architecture get released ||
| [molecule-attention-transformer](https://github.com/lucidrains/molecule-attention-transformer) | 2020 | November | 30 | Pytorch reimplementation of Molecule Attention Transformer, which uses a transformer to tackle the graph-like structure of molecules ||
| [pixel-level-contrastive-learning](https://github.com/lucidrains/pixel-level-contrastive-learning) | 2020 | November | 20 | Implementation of Pixel-level Contrastive Learning, proposed in the paper "Propagate Yourself", in Pytorch ||
| [hamburger-pytorch](https://github.com/lucidrains/hamburger-pytorch) | 2020 | November | 11 | Pytorch implementation of the hamburger module from the ICLR 2021 paper "Is Attention Better Than Matrix Decomposition" ||
| [lightweight-gan](https://github.com/lucidrains/lightweight-gan) | 2020 | November | 11 | Implementation of 'lightweight' GAN, proposed in ICLR 2021, in Pytorch. High resolution image generations that can be trained within a day or two ||
| [AoA-pytorch](https://github.com/lucidrains/AoA-pytorch) | 2020 | November | 7 | A Pytorch implementation of Attention on Attention module (both self and guided variants), for Visual Question Answering ||
| [isab-pytorch](https://github.com/lucidrains/isab-pytorch) | 2020 | October | 26 | An implementation of (Induced) Set Attention Block, from the Set Transformers paper ||
| [x-transformers](https://github.com/lucidrains/x-transformers) | 2020 | October | 24 | A simple but complete full-attention transformer with a set of promising experimental features from various papers ||
| [deep-linear-network](https://github.com/lucidrains/deep-linear-network) | 2020 | October | 16 | A simple implementation of a deep linear Pytorch module ||
| [learning-to-expire-pytorch](https://github.com/lucidrains/learning-to-expire-pytorch) | 2020 | October | 11 | An implementation of Transformer with Expire-Span, a circuit for learning which memories to retain ||
| [memformer](https://github.com/lucidrains/memformer) | 2020 | October | 10 | Implementation of Memformer, a Memory-augmented Transformer, in Pytorch ||
| [lambda-networks](https://github.com/lucidrains/lambda-networks) | 2020 | October | 8 | Implementation of LambdaNetworks, a new approach to image recognition that reaches SOTA with less compute ||
| [performer-pytorch](https://github.com/lucidrains/performer-pytorch) | 2020 | October | 3 | An implementation of Performer, a linear attention-based transformer, in Pytorch ||
| [vit-pytorch](https://github.com/lucidrains/vit-pytorch) | 2020 | October | 3 | Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch ||
| [global-self-attention-network](https://github.com/lucidrains/global-self-attention-network) | 2020 | October | 2 | A Pytorch implementation of Global Self-Attention Network, a fully-attention backbone for vision tasks ||
| [phasic-policy-gradient](https://github.com/lucidrains/phasic-policy-gradient) | 2020 | September | 27 | An implementation of Phasic Policy Gradient, a proposed improvement of Proximal Policy Gradients, in Pytorch ||
| [all-normalization-transformer](https://github.com/lucidrains/all-normalization-transformer) | 2020 | September | 9 | A simple Transformer where the softmax has been replaced with normalization ||
| [kronecker-attention-pytorch](https://github.com/lucidrains/kronecker-attention-pytorch) | 2020 | August | 27 | Implementation of Kronecker Attention in Pytorch ||
| [denoising-diffusion-pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch) | 2020 | August | 26 | Implementation of Denoising Diffusion Probabilistic Model in Pytorch ||
| [marge-pytorch](https://github.com/lucidrains/marge-pytorch) | 2020 | August | 24 | Implementation of Marge, Pre-training via Paraphrasing, in Pytorch ||
| [omniboard](https://github.com/lucidrains/omniboard) | 2020 | August | 20 | Web-based dashboard for Sacred ||
| [mlm-pytorch](https://github.com/lucidrains/mlm-pytorch) | 2020 | August | 14 | An implementation of masked language modeling for Pytorch, made as concise and simple as possible ||
| [electra-pytorch](https://github.com/lucidrains/electra-pytorch) | 2020 | August | 4 | A simple and working implementation of Electra, the fastest way to pretrain language models from scratch, in Pytorch ||
| [attention-tensorflow-mesh](https://github.com/lucidrains/attention-tensorflow-mesh) | 2020 | August | 1 | Tools for building attention networks for Tensorflow Mesh ||
| [conformer](https://github.com/lucidrains/conformer) | 2020 | July | 26 | Implementation of the convolutional module from the Conformer paper, for use in Transformers ||
| [memory-compressed-attention](https://github.com/lucidrains/memory-compressed-attention) | 2020 | July | 25 | Implementation of Memory-Compressed Attention, from the paper "Generating Wikipedia By Summarizing Long Sequences" ||
| [unet-stylegan2](https://github.com/lucidrains/unet-stylegan2) | 2020 | July | 25 | A Pytorch implementation of Stylegan2 with UNet Discriminator ||
| [mixture-of-experts](https://github.com/lucidrains/mixture-of-experts) | 2020 | July | 13 | A Pytorch implementation of Sparsely-Gated Mixture of Experts, for massively increasing the parameter count of language models ||
| [scattering-compositional-learner](https://github.com/lucidrains/scattering-compositional-learner) | 2020 | July | 12 | Implementation of Scattering Compositional Learner in Pytorch ||
| [memory-transformer-xl](https://github.com/lucidrains/memory-transformer-xl) | 2020 | July | 10 | A variant of Transformer-XL where the memory is updated not with a queue, but with attention ||
| [local-attention](https://github.com/lucidrains/local-attention) | 2020 | July | 5 | An implementation of local windowed attention for language modeling ||
| [mogrifier](https://github.com/lucidrains/mogrifier) | 2020 | July | 5 | Usable implementation of Mogrifier, a circuit for enhancing LSTMs and potentially other networks, from Deepmind ||
| [slot-attention](https://github.com/lucidrains/slot-attention) | 2020 | June | 29 | Implementation of Slot Attention from GoogleAI ||
| [linformer](https://github.com/lucidrains/linformer) | 2020 | June | 28 | Implementation of Linformer for Pytorch ||
| [compressive-transformer-pytorch](https://github.com/lucidrains/compressive-transformer-pytorch) | 2020 | June | 24 | Pytorch implementation of Compressive Transformers, from Deepmind ||
| [siren-pytorch](https://github.com/lucidrains/siren-pytorch) | 2020 | June | 19 | Pytorch implementation of SIREN -  Implicit Neural Representations with Periodic Activation Function ||
| [byol-pytorch](https://github.com/lucidrains/byol-pytorch) | 2020 | June | 16 | Usable Implementation of "Bootstrap Your Own Latent" self-supervised learning, from Deepmind, in Pytorch ||
| [nlp](https://github.com/lucidrains/nlp) | 2020 | June | 16 | ðŸ¤— nlp: datasets and evaluation metrics for Natural Language Processing in NumPy, Pandas, PyTorch and TensorFlow ||
| [vector-quantize-pytorch](https://github.com/lucidrains/vector-quantize-pytorch) | 2020 | June | 9 | Vector Quantization, in Pytorch ||
| [axial-positional-embedding](https://github.com/lucidrains/axial-positional-embedding) | 2020 | June | 8 | Axial Positional Embedding for Pytorch ||
| [product-key-memory](https://github.com/lucidrains/product-key-memory) | 2020 | June | 6 | Standalone Product Key Memory module in Pytorch - for augmenting Transformer models ||
| [linear-attention-transformer](https://github.com/lucidrains/linear-attention-transformer) | 2020 | June | 4 | Transformer based on a variant of attention that is linear complexity in respect to sequence length ||
| [axial-attention](https://github.com/lucidrains/axial-attention) | 2020 | May | 28 | Implementation of Axial attention - attending to multi-dimensional data efficiently ||
| [routing-transformer](https://github.com/lucidrains/routing-transformer) | 2020 | May | 22 | Fully featured implementation of Routing Transformer ||
| [contrastive-learner](https://github.com/lucidrains/contrastive-learner) | 2020 | April | 28 | A simple to use pytorch wrapper for contrastive self-supervised learning on any neural network ||
| [compare_gan](https://github.com/lucidrains/compare_gan) | 2020 | April | 22 | Compare GAN code. ||
| [sinkhorn-transformer](https://github.com/lucidrains/sinkhorn-transformer) | 2020 | April | 3 | Sinkhorn Transformer - Practical implementation of Sparse Sinkhorn Attention ||
| [pytorch-optimizer](https://github.com/lucidrains/pytorch-optimizer) | 2020 | March | 2 | torch-optimizer -- collection of optimizers for Pytorch ||
| [memcnn](https://github.com/lucidrains/memcnn) | 2020 | January | 29 | PyTorch Framework for Developing Memory Efficient Deep Invertible Networks ||
| [RevTorch](https://github.com/lucidrains/RevTorch) | 2020 | January | 29 | Framework for creating (partially) reversible neural networks with PyTorch ||
| [AdaMod](https://github.com/lucidrains/AdaMod) | 2020 | January | 24 | Adaptive and Momental Bounds for Adaptive Learning Rate Methods. ||
| [openprotein](https://github.com/lucidrains/openprotein) | 2020 | January | 24 | A PyTorch framework for prediction of tertiary protein structure ||
| [reformer-pytorch](https://github.com/lucidrains/reformer-pytorch) | 2020 | January | 9 | Reformer, the efficient Transformer, in Pytorch ||
| [stylegan2-pytorch](https://github.com/lucidrains/stylegan2-pytorch) | 2020 | January | 9 | Simplest working implementation of Stylegan2, state of the art generative adversarial network, in Pytorch. Enabling everyone to experience disentanglement ||
| [jax](https://github.com/lucidrains/jax) | 2020 | January | 7 | Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more ||
| [tensorflow](https://github.com/lucidrains/tensorflow) | 2019 | June | 19 | An Open Source Machine Learning Framework for Everyone ||
| [tweet-stance-prediction](https://github.com/lucidrains/tweet-stance-prediction) | 2019 | April | 26 | Applying NLP transfer learning techniques to predict Tweet stance ||
| [stylegan](https://github.com/lucidrains/stylegan) | 2019 | February | 5 | StyleGAN - Official TensorFlow Implementation ||
| [tinygbt-js](https://github.com/lucidrains/tinygbt-js) | 2018 | August | 16 |  A Tiny, Pure Javascript implementation of Gradient Boosted Trees. ||
| [liquid-conway](https://github.com/lucidrains/liquid-conway) | 2017 | March | 16 | Liquid simulator based on Conway's Game of Life ||
| [arxiv-sanity-preserver](https://github.com/lucidrains/arxiv-sanity-preserver) | 2017 | February | 13 | Web interface for browsing, search and filtering recent arxiv submissions ||
| [lb_pool](https://github.com/lucidrains/lb_pool) | 2016 | October | 3 | HTTP client load balancer with retries ||
| [a-painter](https://github.com/lucidrains/a-painter) | 2016 | September | 20 | Paint in VR in your browser. ||
| [vectorious](https://github.com/lucidrains/vectorious) | 2016 | February | 24 | A high performance linear algebra library. ||
| [coffee-genetic-algorithm](https://github.com/lucidrains/coffee-genetic-algorithm) | 2015 | July | 7 | a simple genetic algorithm written in coffeescript ||
| [coffee-neural-network](https://github.com/lucidrains/coffee-neural-network) | 2015 | July | 7 | a simple neural network in coffeescript ||
| [recurrentjs](https://github.com/lucidrains/recurrentjs) | 2015 | June | 4 | Deep Recurrent Neural Networks and LSTMs in Javascript. More generally also arbitrary expression graphs with automatic differentiation. ||
